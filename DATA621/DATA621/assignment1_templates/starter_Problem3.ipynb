{"cells":[{"cell_type":"markdown","metadata":{"id":"fWOnAzAM47ZV"},"source":["# Problem 3 (tot 50 points)\n","\n","\n","**Fashion MNIST**\n","\n","In this assignment, you will develop a Multi-Layer (2 layers) Perceptron (MLP) to classify the Fashion MNIST dataset from scratch. You can take advantage of the previous questions as a starting point.\n","\n","\n","The [Fashion MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist) dataset consists of 60,000 examples and a test set of 10,000 examples. Each example is a grayscale image of a fashion item, sized $28 \\times 28$ pixels, associated with one of the 10 classes. The 10 classes represent different types of clothing or accessories, such as T-shirts, trousers, dresses, etc. You may have to download the dataset from the link. There are both Training and testing dataset separately. Make sure to download both the files and then use them for this problem.\n","\n","## Instructions for Submission\n","\n","1. Please submit as a notebook file named `Problem_3_First_LastName.ipynb`.\n","\n","2. There is no need of installing additional libraries, than the ones already provided in the starter code.\n","\n","3. Make sure to use Markdown cells to explain your code and provide any necessary comments.\n","\n","4. Before every question, please add a Markdown cell with the question number and if possible brief description of the task.\n","\n","5. Make sure to save the exact state of your notebook before submitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z18mcQ2G47ZX"},"outputs":[],"source":["!pip install numpy pandas matplotlib numpy scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRHFv7fk47ZY"},"outputs":[],"source":["import numpy as np\n","##########################\n","### MODEL\n","##########################\n","\n","def relu(#PARAMS\n","         ):\n","    # FILL THIS\n","    pass\n","def derivative_relu(#PARAMS\n","                    ):\n","    # FILL THIS\n","    pass\n","def logistic(#PARAMS\n","             ):\n","    # You may want to clip the values to avoid overflow\n","    # FILL THIS\n","    pass\n","def derivative_logistic(#PARAMS\n","                        ):\n","    # FILL THIS\n","    pass\n","def tanh(#PARAMS\n","         ):\n","    # FILL THIS\n","    pass\n","def derivative_tanh(#PARAMS\n","                    ):\n","    # FILL THIS\n","    pass\n","\n","# Below you are provided with the softmax function and its derivative along with cross-entropy loss function\n","# Use these functions in the NeuralNetMLP class\n","def softmax(logits):\n","    \"\"\"\n","    Compute softmax for each set of logits with numerical stability.\n","\n","    Parameters:\n","    logits (ndarray): Raw logits of shape (N, C) where N is the number of samples and C is the number of classes.\n","\n","    Returns:\n","    ndarray: Softmax probabilities of shape (N, C).\n","    \"\"\"\n","    # Check for NaN or inf values in logits (optional for debugging)\n","    if np.isnan(logits).any() or np.isinf(logits).any():\n","        print(\"Warning: Logits contain NaN or infinity values!\")\n","\n","    # Clip the logits to avoid extremely large or small values that might cause overflows in exp\n","    logits = np.clip(logits, -100, 100)\n","\n","    # Numerically stable softmax\n","    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Subtract max for numerical stability\n","    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n","\n","\n","def ce_loss(probs, labels):\n","    \"\"\"\n","    Compute the cross-entropy loss.\n","\n","    probs: numpy array of shape (N, C) where N is the number of samples and C is the number of classes.\n","           This is the output of the softmax function, i.e., predicted probabilities.\n","\n","    labels: numpy array of shape (N, C), one-hot encoded true labels.\n","\n","    Returns:\n","    loss: scalar, the average cross-entropy loss over all samples.\n","    \"\"\"\n","    # Clip the probabilities to prevent log(0) which can result in NaN values\n","    probs = np.clip(probs, 1e-12, 1.0)\n","    # Compute the cross-entropy loss\n","    loss = -np.sum(labels * np.log(probs)) / labels.shape[0]\n","    return loss\n","\n","def derivative_ce(probs, labels):\n","    \"\"\"\n","    Compute the derivative of the cross-entropy loss w.r.t. logits (softmax input).\n","\n","    probs: numpy array of shape (N, C), the predicted probabilities (output of softmax).\n","    labels: numpy array of shape (N, C), one-hot encoded true labels.\n","\n","    Returns:\n","    dL/dz: numpy array of shape (N, C), the derivative of the loss w.r.t the logits.\n","    \"\"\"\n","    return (probs - labels) / labels.shape[0]\n","\n","class InitializeModel:\n","    def __init__(self, random_seed=123):\n","        print(\"Model initialization\")\n","        self.random_seed = random_seed\n","        self.rng = np.random.RandomState(random_seed)\n","\n","class NeuralNetMLP(InitializeModel):\n","\n","    def __init__(self, num_features, num_hidden1, num_hidden2, n_out, random_seed=123):\n","        super().__init__(random_seed)\n","\n","        # INITIALIZE YOUR MODEL\n","\n","        # Defining some metrics for book keeping and looking into performance of model\n","        # Feel free to add more metrics or modify them\n","        self.metrics = {\"loss\": {\"training\": [], \"validation\": []},\n","                        \"accuracy\": {\"training\": [], \"validation\": []}}\n","\n","        print (\"Model initialized\")\n","        print (\"First hidden layer weights: \", None)\n","        print (\"Second hidden layer weights: \", None)\n","        print (\"Output layer weights: \", None)\n","\n","    def forward(self, x):\n","        # FILL THIS\n","        pass\n","\n","    def backward(self #PARAMS\n","                 ):\n","        # FILL THIS\n","        pass\n","\n","\n","\n","#model = NeuralNetMLP(num_features=X.shape[-1],\n","#                     num_hidden1=1000,\n","#                     num_hidden2=1000,\n","#                     num_classes=n_out)"]},{"cell_type":"markdown","metadata":{"id":"QjScp50K47ZY"},"source":["## K-fold cross-validation\n","\n","Make sure to implement k-fold cross-validation in the `train` function.\n","\n","## WanB dashboard (Bonus)\n","\n","Look into [docs](https://docs.wandb.ai/) for more information. Look into the [quick start](https://docs.wandb.ai/quickstart) guide for a quick introduction.\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Js_iaBgP5LvX"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"env_NNDL2024","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}